Algorithm Analysis
===================

== Asymtotic analysis

=====
n! << 2^n^  << n^3^  << n^2^  << n log n << n << log n << 1
=====

== Strings

=== Substring

==== Boyer-Moore

* Explained in Goodrich/Tamassia/Goldwasser
* Idea is to build a hash for each char, the last index it appears
  in pattern. If a char is non-existant, then make it -1.
* So, when a mismatch happens, we check if the mismatching char is
  present in sub-string or not. If not we slide the sub-string
  fully past there. If not, we slide the string till the index
  we know its present.
* Since we store the last index, we start matching from end of
  pattern to beg of pattern - so that the last-index ajusting
  works.

==== KMP (Knuth Morris Pratt) Pattern Searching

* Explained in http://www.geeksforgeeks.org/searching-for-patterns-set-2-kmp-algorithm/[geeks for geeks]
+
****
lps[i] = the longest proper prefix of pat[0..i] 
              which is also a suffix of pat[0..i]. 
****
+
* Having built this lps array, when a mismatch happens at a index, move the substring by the lps[i-1]th
  spaces - as we know the prefix of that size is already found
* But trick is to build lps array! I didn't get this fully. See at geeks site.
+
----
def compute kmp fail(P):
  '''Utility that computes and returns KMP fail list.'''
  m = len(P)
  fail = [0]*m           # by default, presume overlap of 0 everywhere
  j = 1
  k = 0
  while j < m:           # compute f(j) during this pass, if nonzero
    if P[j] == P[k]:     # k + 1 characters match thus far
      fail[j] = k + 1
      j += 1
      k += 1
    elif k > 0:          # k follows a matching prefix
      k = fail[k−1]      #  <-- This is what i dont follow
                         #      Also note no increment of j
    else:                # no match found starting at j
      j += 1
  return fail
----

== Sorting and Related Algorithms

=== Counting Inversions

* This is same as merge sort. As part of merging, just count inversions.
* Remember, u should also do sorting alongside.

== Binary Trees

* A tree is also a graph. Any undirected graph, where any 2 vertices are
  connected in exactly one path is a tree. In other words any connected
  graph, without simple cycles is a tree.

=== Diameter of a binary tree

* Treating a tree as a graph, the diameter is the longest path from one
  leaf node to another leaf node.
* To find a the diameter of a tree, try this algorithm as 
  explained in this
  http://www.geeksforgeeks.org/diameter-of-a-binary-tree/[geek-for-geek]
* If you are given a graph and u want to find the diameter, try this
  http://stackoverflow.com/questions/25649166/linear-algorithm-of-finding-tree-diameter[stack-overflow]
  idea.

== Binary Search Tress

* Each node is such that all the left nodes are lesser and right nodes are higher.
* Note that if tree isnt balanced, there are many ways of building a tree for the
  same set of numbers. Balancing is a different act.
* Options for balancing
** Red-Black Trees
** AVL Trees
** Splay Tress
** B Trees

=== Operations comparison

[options="header"]
|=========
| Operations                   | Running time for BST  | Running time for sorted array  | Comment
| Search                       | O(log n)              | O(log n)                       | Same
| Select ith order statistic   | O(log n)              | O(1)                           | Bst costlier
| Min/Max                      | O(log n)              | O(1)                           | Bst costlier
| Predecessor/Successor        | O(log n)              | O(1)                           | Bst costlier
| Rank                         | O(log n)              | O(log n)                       | Same
| Output in sorted order       | O(n)                  | O(n)                           | Same
| Insert                       | O(log n)              | O(n)                           | Bst cheap
| Delete                       | O(log n)              | O(n)                           | Bst cheap
|=========

=== Operations

==== Min/Max

* Start from root.
* Keep following left(or right) points until there is no left pointer. Return
  the leaf node you are at, that has no left(right) child, which will be the minimum so far.

==== Predecessor of a key k. (Turn words for Successor)

* If left-child is non-empty, get the max element of this sub-tree (by travelling right pointers)
* If left-child is empty, keep travelling up parent pointers, until u find a parent that is lesser
  in value.
** This node is higher than parent if its a right-child. In that case, parent is the lesser
   valued node of interest.
*** Why can't there be an element between this parent and this node? [red yellow-background]#Find out#
** This node is lesser than parent (then parent is higher) and we are left child. So, keep moving up
   until the current-node is a right-child.

==== Deleting a node

* Search node first if just key is given.
* If node has no children, delete node.
* If node has just one child, just replace that child in place of the node.
* If node has 2 children, comupute k's predesscor (That is the right most node of left child)
** Swap that predessor node and this node and remote it off.
** This will equally work by using the successor.

==== Selecting ith order statistic and Finding Rank

* Store the size of tree below in every node.
** Let size(a) be the size of tree at a given node.
** By defn, sizeof(a) = 1 + sizeof(a->left) + sizeof(a->right)
** sizeof(a->left) = 0 if a has no left child.
* Algo
+
----
def get_ith_statistic(i,root):
  ''' i is from 0 to n-1 '''
  a=root
  if i == sizeof(a->left):
    return a
  if i < sizeof(a->left):
    get_ith_statistic(i,a->left)
  else:
    get_ith_statistic(i-(sizeof(left))-1,a->right)
----
* Rank (My own..not vetted from sources)
** Find node first
** Rank is simply sizeof(a->left)+1

==== Rotations

* Rotations are fundamental to any balancing of binary tress.
* Its basically rewiring of pointers.
+
----
     y                   x
    / \    right       /  \
  x     C  ----->     A    y
 / \       <-----         / \
A   B      left          B   C

Note that in both arrangements:   A < x < B < y < C
----


=== Red Black Trees

==== Rules

* Each node is red or black
* Root is black
* No 2 reds come in row
** [red node => black children]
* Every root->NULL (leaf path, like in unsuccessful search) will have equal
  number of black nodes.

==== Observations

* Because of the rules, there can be utmost log(n+1) black nodes in every path.
* Because of the rules, there can be utmost log(n+1) red nodes stuffed in between
  black nodes.
* So the tree is utmos 2 log (n+1) height tall.
* Note a fully black tree is also a valid RB tree.

==== Inserting in a RB Tree

* We will refer the node to insert as N, parent as P, grand-parent as G and
  sibling of P as uncle(U)
* We just Regular node insert in a BST and color this node N red (except in
  case 1).
* Case 1
** Tree is empty. N is the root and is black
* Case 2: Parent(P) is black.
** No property is disturbed by adding a Red child N.
* Case 2: Parent and Uncle are both Red.
** If parent(P) is red, we are violating - as there are 2 consecutive reds. However,
   note that here grand-parent(G) definitely exists and is black, by RB-tree rules.
   Root has to be black - so there is definitely one parent to P. G, by definition
   has to be Black, as P is already red.
+
----
         G-B
       /   \
     P-R    U-R
     /
   N-R             (N could be placed anywhere in the 4 slots, its holds good)
----
+
** recolor P and U as Black and G as red. This wont break the rule-4 (equal
    blacks on path, we are only bringing black one level below)
+
----
         G-R
       /   \
     P-B    U-B
     /
   N-R             (N could be placed anywhere in the 4 slots, its holds good)
----
+
** But rule-3 of no red's together may or may not be broken. If we are lucky,
   G's parent is Black, then we can stop. If not, we need to repeat this
   treating G as the new node added. (Beneath G rule-4 is preserved)
* Case 4: Parent is R, and Uncle is B. N is added as a left child of parent,
  which is a right child of G. (Or vice versa)
** (Self note:) This arrangement can never happenw when we start, as at a level
   how can there originally be a P-node as leaf and red, with its sibling a
   black. Isn't this violiating rule-4? Perhaps this is a possible configuration
   as we iterate on case-3 above and proceed to top?
** In this case, we rotate N and P and make it same as case-5 below
+
----
         G-B                    G-B
       /   \                  /   \
     P-R    U-B   ==>       N-R    U-B
    /  \     / \           /  \     / \
   A   N-R  D   E        P-R   C   D   E
       / \               /\
      B   C             A  B
----
+
* Case 5: Parent is R, and Uncle is B. N is added as a left child of parent,
    which is a left child of G.
** Rotate as shown below. Note that originally G-U path had 2 blacks and that
   is still maintained. This wasn't violoatd when we started with (what was
   violated was 2 consecutive reds and that is now fixed)
+
----
         G-B                         P-R
       /   \                       /   \
     P-R    U-B                  N-R    G-R
    /  \     / \     ==>        /  \     / \
  N-R   C   D   E              A    B   C   U-B
  /\                                       / \
 A  B                                     D   E
----

==== Deletion


== Terneray Search Tress

* Each node has a data-member (one char - or trie's equivalent), and an optional end-of-word marker.
  The end of word doesn't necessarily imply termination of search. For eg, cat, cats will
  have cat's t having end-of-word, but there is also a cats
* Each Node has a {lo,eq,hi}-kid pointer. I personally want to call it low-peer, high-peer and child
  pointers
* Operations are insert, search, get-next-char, get-next-string, get-all-substrings.

=== Links to read

* http://www.drdobbs.com/database/ternary-search-trees/184410528[Dobbs journal]

== Heaps

* A simple binary tree-like looking structure where the only condition is that
  the parent node is less(or greater) than its children. This ensure the root
  is the min(or max) element
** The tree is naturally full-complete or partially complete
** There are many ways of arranging a given set of numbers in a heap - as the
   only condition is the heap-property.
* Two main operations that it supports
** insert
** extract-min
* The next operations are
** heapify
*** This will prepare a heap from a random collection of items.
*** The standard way will take n $$*$$ log n time. But, there is a
    slick way to do it in O(n), if all numbers are available apriori.
+
This is explained in this http://stackoverflow.com/a/9755805/2587153[stack overflow answer]
and is further explained why heapify is O(n) and heap-sort is still O(nlogn) in this
http://stackoverflow.com/a/18742428/2587153[answer]
+
**** The idea is that when you heapfiy from the leaf nodes, n/2 nodes have 
     0 operations, n/2 have 1 operation, n/4 have 2,.. and only the root has
     log(n) operations. So is accepted to be O(n)
+
****
(0 * n/2) + (1 * n/4) + (2 * n/8) + ... + (h * 1).
****
+
**** However, in case of heap sort itself, work for each node decreases by size of 1.

=== Uses of heap
** Heap sort
** find median in a collection of number
** Any algo that needs to keep picking minimum, like Dijkstra's shortest path
   algo

=== Implemetation

* Heaps are usually implemented as trees. But array way of representing heaps are 
  more common.
* In a 0-based array
+
----
For, index i
2i+1 and 2i+2 are its 2 children
i-1/2 is its parent
----
+
* For sifting up, add a node to the end of tree. Shift it up, till it is at the
  right position. This is part of the heapfiy operation.
* Extract min is simply taking the root first. Now swap the last node of the
  heap as root and bubble down.

== Hashes

* Every effective of lookups / insertions / deletions
* Can't handle the sort/ordering of keys
** min-max, next/prev, select/rank are out.
* Typically the Universe(U) of all possible elements is too huge and we deal
  with a small subset of elements(S) at a given time.
** Hash has number of bins that is comparable to the cardinality of S
* It takes just sqrt(n) elements to have a 50% probability of collision, even if
  n elements have equal probability of coming.
** What that means is , even if u have 10K buckets in your hash, and ur universe
   is pretty HUGE and there is a probability for an element to take any of the
   10K buckets, then it still takes only 100+ elements to have a 50% probability
   for collision!
* Prevelant options for handling collisions
** Chaining
** Open addressing
+
The algo-video is pretty hazy here. It says about a hash-sequence which gives a
sequence of hash-funcitons to suggest buckets
+
*** Linear probing
+
Just keep searching n+1....Nth.0th..n-1th slots after nth slot is taken.
+
*** Double hashing.
+
Improvement over Linear probing, where a second hash-function gives an offset.
This liner probing has a offset of always 1. But how to do stop searching on
a lookup?!!
+
** The only adv of open-addressing is that it is space-effective. (doest waste
   linked list keeping)
* Load of a Hash-table
+
----

  Load = No of elements
        -----------------
         No of buckets
----
For a load > 1, open-addressing is not possible. Only chaining is possible
+
* Another bell and whistle is to adjust size of hash table.

=== Hash functions

* Easy to make mistakes
** For eg, taking 3 MSB numbers for telephones is a very bad choice.  Bad choices
   may expose patterns of numbers that aren't visible to naked eye.
** Memory address are mostly always multple of 4. So if the lsb 2  bits are used
   as is, 3/4th of hash-buckets will be unused!
** If all data is multple of N and hash-buket-number is a multiple of N, we may
   have unfilled buckets.
* For a simple modulus like hash-function, choosing number of buckets(n) to be a
  prime closest to our desired N range.
** The prime shouldnt be too close to a power of 2 or power of 10

=== Universal Hashing



== Graphs

* Represents pair-wise relationship among objects
* Terminology
** Vertices or nodes
** Edges
*** Careful. Dont confuse edge (having smaller spelling for a vertex, having a bigger
    spelling!)
** Directed or Undirected edges
*** In directed, first is tail and second is head. That is direction is from tail to head.
** *Cuts*
*** A Cut in a graph is a split of vertices into 2 non-empty groups(A and B).
*** For undirected, Crossing edges are those that have one end-point each in A and B
*** For Directed, Crossing edges are those that have tail in A, head in B
** Parallel edges are those that have same origin and destination vertex. This may
   or may not be meaningful to a given problem
** Typically m is number of edges, and n is number of vertices. (Mnemonic: m>>n, we
   have far more edges than there are vertices. Number of lines in m(3) is more than n(2)!
   Alternatively, m is O(n) to upto O(n^2^) to classify sparse/dense)
** A degree of a vertex, is referred as the number of edges that start out from that
   vertex.


=== Numerical Facts

* If there are n vertices, assuming no parallel edges, there should be minimum of
  n-1 edges to have all the vertices connected (in one line) and utmost ~n~C~2~ = n(n-1)/2
  number of edges (where all edge is connected to the other edge) in a undirected graph
* If there are n vertices in a graph, we can have up 2^n^-2 possible cuts for this graph.
  Each vertex can be in either set A or B independant of the choice of other vertices. We
  just subtract 2 as we can't have all vertices in each set.

=== Graph Representations

==== Adjacency Matrix

* We have nxn matrix (verticesxvertices matrix).
* Each non-primary diagnal represents a possible Edge. Its 0/1 based on if that edge exists.
* Add bells and whistles to what the matrix element is to accomodate directed (+ve/-ve),
  parallel edges, weighted edges
* Super waste of memory for a sparse graph.

==== Adjancenty List

* Algo course style
** Have 2 different lists - one for vertices and one for edges
** They cross reference each other.
*** Each edge points to its 2 vertices. This way edge struct is of fixed size.
*** Each vertex points to all edges incident on it. The vertex thus should have
    a list of edge-pointers.
*** Note that the sum of cross-references from edges to vertices is exactly
    same as vertices to edges. The edges to vertices are exaclty 2 per edge,
    while in vertices to edges, it varies on degree of each vertex.

* Skiena book style
** Kind of a 2D linked list.
** We have a linked-list of vertices.
** For each vertex, we have a list of edges that originate from that vertex.
** For unidirected, the edge appears twice, once is each vertex's list. For directed
  graph, it appears in the vertex which is its tail.
** Here we show the list of vertices as an array and the vertices as linked-list.
+
----
#define MAXV 1000 /* maximum number of vertices */

typedef struct {
  int y;                 /* adjacency info */
  int weight;            /* edge weight, if any */
  struct edgenode *next; /* next edge in list */
} edgenode;

typedef struct {
  edgenode *edges[MAXV+1]; /* adjacency info */
  int degree[MAXV+1];      /* outdegree of each vertex */
  int nvertices;           /* number of vertices in graph */
  int nedges;              /* number of edges in graph */
  bool directed;           /* is the graph directed? */
} graph;
----

The Skienna book style and algo-course styles are kind of same. In both ways, u can
walk over vertices and then for each vertex walk over its edges. Just that the algo-course
suggests to keep the actual vertices and edges separately in lists of their own.

===== Comparision

[options="header"]
|=======================
|Comparison                             | Winner
|Faster to test if (x,y) is in graph?   | adjacency matrices
|Faster to find the degree of a vertex? | adjacency lists
|Less memory on small graphs?           | adjacency lists (m + n) vs. (n2)
|Less memory on big graphs?             | adjacency matrices (a small win)
|Edge insertion or deletion?            | adjacency matrices O(1) vs. O(d)
|Faster to traverse the graph?          | adjacency lists Θ(m + n) vs. Θ(n2)
|Better for most problems?              | adjacency lists
|=======================

=== Graph classifications

* Directed, Undirected
* Sparse, Dense
** Sparse has edge-number closer to the linear bound (n-1), while dense matrix is
   where edge-number is closer to upper bound ~n~C~2~

=== General Algorithms in a Graph

==== Minimum cuts

Given a graph, find the cut that has the minimum number of cross-over edge (Min-cut)
This is useful, to find closesly related vertices in a graph.

===== Karger Algorithm

The solution allows parallel edges for this graph. This goes as follows:

* Keep proceeding till the node-count reduces to 2.
* In every iteration, *randomly* pick an edge and collapse the 2 vertices that
  it connects into one fused-super-vertex. Remove this chosen edge.
* Remove any edges that start and end at same-edge.
* When you are left with 2, all vertices part of each fused/orig vertex is
  the resulting graph-cut.

But this is just a random algo. There is no guarantee that the resulting cut
is a min-cut.

===== Analysis of this algorithem

* If a edge that should remain as part of min-cut, ends up getting randomly
  chosen, then the algo will fail.
* But if run a few times, this algorithem will succeed with a high degree of
  probability.

==== Graph Traversal

* Before traversal, we can mark each node as one of the 3 states
** Undiscovered
** Discovered
** Processed
* Most search graph algorithms consider one vertex as the source/start vertex.

===== Breadth First Traversal

* Note the presence of a Queue in BFS
* It grabs territory layer by layer from source vertex.

.Skienna code
----
BFS(G,s)
  for each vertex u ∈ V [G] − {s} do
    state[u] = "undiscovered"
    p[u] = nil, i.e. no parent is in the BFS tree
  state[s] = "discovered"
  p[s] = nil
  Q = {s}
  while Q ≠ ϕ do
    u = dequeue[Q]                   /* Note that u is already discovered *
    process vertex u as desired       * However processing of u happens now */
    for each v ∈ Adj[u] do
      process edge (u,v) as desired  /* For undirected graph, this edge may be already processed
                                        So, if u want only one time, track that as well */
      if state[v] = "undiscovered" then
        state[v] = "discovered"
        p[v] = u                    /* This parent path from v to source s, is the shortest path from s to v
                                       for undirected graphs. For directed graphs, there may be back-pointing
                                        edges! */

        enqueue[Q,v]
    state[u] = "processed"
----

* Whatever is marked processed at the end of BFS is what is reacheable from s.

*Applications*

* Find all connected nodes to a given graph.
** You can keep all nodes in a bigger outer queue.
** Start with one node, and do a BFS from here. You will pick all nodes that is
   connected with this. Dequeue from outer queue as you meet nodes.
** Keep doing BFS, till outer queue is empty. That way you get all groups in the
   graph.
** This could be done DFS way also (my own observation!)
* Find distance of each vertex from source vertex s.
** This is easily achived by storing the distance in each vertex - as part of processing
   of node.
* Find the path to each vertex from source vertex s.
** If we build the parent of each node info in our BFS, we can use recursive approach
   to build the root from the parent info.
+
----
parents_arr=[...]
find_parent(parents_arr, parent_desired, node)
{
  if ( node == parent || parents_arr[node] == -1) {
    printf("%d",parent_desired);
  } else {
    find_parent(parents_arr, parent_desired, parents_arr[node]);
    printf("%d",node);
  }
}
----
+
* Find if a graphs is bipartite (can you assign vertices either of 2 colors, such
  that no two adjancent vertex is of same color)
** Keep running BFS and see if you can successfully finish BFS.

===== Depth First Traversal

* It just goes all in into one path
* We can technically modify BFS slightly by replaceing Queue with stack, or
  leverage recursion to naturally achieve our stacking.

.Skienna code Vs Standford-algo-course-code
----
time = 0          /* is a global var */    |
DFS(G,u)                                   |    DFS(G,u)
  state[u] = "discovered"                  |     state[u] = "discovered"
  process vertex u if desired              |        ... early_process_vertex ...
  entry[u] = time                          |        ...
  time = time + 1                          |        ...
  for each v ∈ Adj[u] do                   |     for each v ∈ Adj[u] do
    process edge (u,v) if desired          |        ... process_edge (u,v)
    if state[v] = "undiscovered" then      |        if state[v] = "Undiscovered" then
      p[v] = u                             |          ...
      DFS(G,v)                             |          DFS(G,v)
  state[u] = "processed"                   |
  exit[u] = time                           |
  time = time + 1                          |
----

* The notion of parent is looking superfluous to me.
* The time spent at a parent will be a superset of time
  spent at children. Not sure how otherwise time is useful.

====== Applications

* Find cycles
** DFS by its nature, can classify a edge into tree edges and back
   edges. Edge that explores new vertex is a tree edge, while an
   edge that goes back into a ancestor is a back-edge
** A acyclic graph is one, that has no back-edge during a traversal
* Topological sort for directed graph
** Topological sort is the ordering of what should be completed before
   another.
** Topological order is possible only if there are no cycles. Further
   this implies that acyclic graphs should have atleast one sink vertex.
*** *A sink vertex* is a vertex that has no outgoing edges. If there
    are no sink vertices in a graph, then it definitely cyclic.
** You can start DFS at any vertex. You will end up walking all nodes
   from this. But some vertices may have higher precendence(before)
   than the chosen vertex.
** THere are 2 solutions.
*** First
**** Start with a sink vertex(you need to find this some how). Give
     this the value N (no of vertices in graph). This has to be done
     last by definition.
**** Now remove that node from Graph. Repeat the algorithm. (Due to
     loss of this node, there will be new sink vertex(ices) created
*** Second / slick way.
**** This is same as DFS algo. Just have a current-label extra with
     initial value as N, number of edges
**** Start DFS from any node. Once you hit the depth of recursion,
     which is end of for-loop, assign the current-label value to
     that node.
**** This will neatly assign the Nth value for the first sink you
     discover and back-track from there and so on.
+
----
topological_sort(G)
  all vertices = "Undiscovered"
  current_label = N  /* number of vertices */
  for each vertex u
    if u is "Undiscovered":
      DFS(G,u)

DFS(G,u)
 state[u] = "discovered"
 for each v ∈ Adj[u] do
   if state[v] = "Undiscovered" then
      DFS(G,v)
   level-of-u = current_label
   current_label--
----
+
* Articulate edge detection
** Given in skienna.
** I didn't follow it
